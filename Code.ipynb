{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "KKOl39Vt24Zo"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import pandas as pd\n",
    "import scipy as sp\n",
    "import math, copy, random, multiprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9WBtZy1vpNTH"
   },
   "outputs": [],
   "source": [
    "class Rover_architecture(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Rover_architecture, self).__init__()\n",
    "        self.fc1 = nn.Linear(20, 32)  # Input layer (16 units to 32 units)\n",
    "        self.fc2 = nn.Linear(32, 32)  # Hidden layer (32 units to 32 units)\n",
    "        self.fc3 = nn.Linear(32, 2)   # Output layer (32 units to 2 units)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "class Drone_Excavator_architecture(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Drone_Excavator_architecture, self).__init__()\n",
    "        self.fc1 = nn.Linear(16, 32)  # Input layer (16 units to 32 units)\n",
    "        self.fc2 = nn.Linear(32, 32)  # Hidden layer (32 units to 32 units)\n",
    "        self.fc3 = nn.Linear(32, 2)   # Output layer (32 units to 2 units)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "dwEwFn-g25ol"
   },
   "outputs": [],
   "source": [
    "class Rover:\n",
    "    def __init__(self, Game_x, Game_y, model=Rover_architecture()):\n",
    "        self.type = 'Rover'\n",
    "        self.position = np.array([np.random.uniform(0,Game_x), np.random.uniform(0,Game_y)])\n",
    "        self.model = model\n",
    "        self.Game_x = Game_x\n",
    "        self.Game_y = Game_y\n",
    "\n",
    "    def compute_quadrant(self, position):\n",
    "      quadrant = 0\n",
    "      if(position[0] <= self.Game_x//2):\n",
    "        if(position[1] <= self.Game_y //2):\n",
    "          quadrant = 1\n",
    "        else:\n",
    "          qaudrant = 3\n",
    "      else:\n",
    "        if(position[1] <= self.Game_y //2):\n",
    "          quadrant = 2\n",
    "        else:\n",
    "          quadrant = 4\n",
    "      return quadrant\n",
    "\n",
    "    def get_next_move(self,sites, agents):\n",
    "      input_data = torch.Tensor(self.compute_site_density(sites)+ self.compute_agent_density(agents))\n",
    "      dx,dy = self.model.forward(input_data)\n",
    "      self.position[0] += dx\n",
    "      self.position[1] += dy\n",
    "      if self.position[0] > self.Game_x: self.position[0] = self.Game_x\n",
    "      if self.position[1] > self.Game_y: self.position[1] = self.Game_y\n",
    "      return [dx,dy]\n",
    "\n",
    "\n",
    "    def rollout_island(self, sites, agents, num_steps):\n",
    "      trajectory = []\n",
    "\n",
    "      for _ in range(num_steps):\n",
    "        # Get the current state observation (site and agent data)\n",
    "        site_density = self.compute_site_density(sites)\n",
    "        agent_density = self.compute_agent_density(agents)\n",
    "        state = torch.Tensor(site_density + agent_density)\n",
    "\n",
    "        # Store the current state\n",
    "        current_state = state.clone()\n",
    "\n",
    "        # Choose an action using the policy (assuming a discrete action space)\n",
    "        action = self.get_next_move(sites,agents)\n",
    "\n",
    "\n",
    "        # Calculate the reward based on the new state\n",
    "        closest_dig_site = self.find_closest_dig_site(sites)\n",
    "        reward = self.reward(closest_dig_site)\n",
    "\n",
    "        # Store the data for this time step\n",
    "        trajectory.append([current_state, action, reward])\n",
    "\n",
    "      return trajectory\n",
    "\n",
    "    def compute_site_density(self, sites):\n",
    "        # Implement equation 3\n",
    "        site_density = [0]*8\n",
    "        for site in sites:\n",
    "            quadrant = site.quadrant-1\n",
    "            distance = np.linalg.norm(self.position - site.position)\n",
    "            if site.type == 'Marked':\n",
    "              site_density[quadrant] += site.value/distance\n",
    "            if site.type == 'Unmarked':\n",
    "              site_density[quadrant+4] += site.value / distance\n",
    "        return site_density\n",
    "\n",
    "    def compute_agent_density(self, agents):\n",
    "        # Implement equation 2 first 4 rover , then excavator, then drone\n",
    "        agent_density = [0]*12\n",
    "        for agent in agents:\n",
    "          distance = np.linalg.norm(self.position - agent.position)\n",
    "          quadrant = self.compute_quadrant(agent.position)-1\n",
    "          if agent.type == \"Rover\": agent_id = 0\n",
    "          elif agent.type == \"Excavator\": agent_id = 1\n",
    "          else: agent_id = 2\n",
    "\n",
    "          if(distance == 0):\n",
    "            continue\n",
    "          agent_density[4*agent_id + quadrant] += 1/distance\n",
    "\n",
    "        return agent_density\n",
    "\n",
    "    def reward(self, closest_dig_site):\n",
    "        return 1/ torch.norm(torch.tensor(self.position, requires_grad=True) - torch.tensor(closest_dig_site.position), p=2).requires_grad_()\n",
    "\n",
    "    def find_closest_dig_site(self, sites):\n",
    "      min = 100000000\n",
    "      best = sites[0]\n",
    "      for i in sites:\n",
    "        dist = np.linalg.norm(self.position-i.position)\n",
    "        if(dist < min):\n",
    "          min = dist\n",
    "          best = i\n",
    "\n",
    "      return best\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "class Excavator:\n",
    "    def __init__(self, Game_x, Game_y, model=Drone_Excavator_architecture()):\n",
    "        self.type = 'Excavator'\n",
    "        self.position = np.array([np.random.uniform(0,Game_x), np.random.uniform(0,Game_y)])\n",
    "        self.model = model\n",
    "        self.Game_x = Game_x\n",
    "        self.Game_y = Game_y\n",
    "\n",
    "    def compute_quadrant(self, position):\n",
    "      quadrant = 0\n",
    "\n",
    "      if(position[0] <= self.Game_x//2):\n",
    "        if(position[1] <= self.Game_y //2):\n",
    "          quadrant = 1\n",
    "        else:\n",
    "          qaudrant = 3\n",
    "      else:\n",
    "        if(position[1] <= self.Game_y //2):\n",
    "          quadrant = 2\n",
    "        else:\n",
    "          quadrant = 4\n",
    "      return quadrant\n",
    "\n",
    "    def get_next_move(self,sites, agents):\n",
    "      input_data = torch.Tensor(self.compute_site_density(sites)+ self.compute_agent_density(agents))\n",
    "      dx,dy = self.model.forward(input_data)\n",
    "      self.position[0] += dx\n",
    "      self.position[1] += dy\n",
    "      if self.position[0] > self.Game_x: self.position[0] = self.Game_x\n",
    "      if self.position[1] > self.Game_y: self.position[1] = self.Game_y\n",
    "      return [dx,dy]\n",
    "\n",
    "\n",
    "    def rollout_island(self, sites, agents, num_steps):\n",
    "      trajectory = []\n",
    "\n",
    "      for _ in range(num_steps):\n",
    "        # Get the current state observation (site and agent data)\n",
    "        site_density = self.compute_site_density(sites)\n",
    "        agent_density = self.compute_agent_density(agents)\n",
    "        state = torch.Tensor(site_density + agent_density)\n",
    "\n",
    "        # Store the current state\n",
    "        current_state = state.clone()\n",
    "\n",
    "        # Choose an action using the policy (assuming a discrete action space)\n",
    "        action = self.get_next_move(sites,agents)\n",
    "\n",
    "\n",
    "        # Calculate the reward based on the new state\n",
    "        closest_dig_site = self.find_closest_dig_site(sites)\n",
    "        reward = self.reward(closest_dig_site)\n",
    "\n",
    "        # Store the data for this time step\n",
    "        trajectory.append([current_state, action, reward])\n",
    "\n",
    "      return trajectory\n",
    "\n",
    "    def compute_site_density(self, sites):\n",
    "        # Implement equation 3\n",
    "        site_density = [0]*4\n",
    "        for site in sites:\n",
    "            quadrant = site.quadrant-1\n",
    "            distance = np.linalg.norm(self.position - site.position)\n",
    "            if site.type == 'Marked':\n",
    "              site_density[quadrant] += site.value/distance\n",
    "        return site_density\n",
    "\n",
    "    def compute_agent_density(self, agents):\n",
    "        # Implement equation 2 first 4 rover , then excavator, then drone\n",
    "        agent_density = [0]*12\n",
    "        for agent in agents:\n",
    "          distance = np.linalg.norm(self.position - agent.position)\n",
    "          quadrant = self.compute_quadrant(agent.position)-1\n",
    "          if agent.type == \"Rover\": agent_id = 0\n",
    "          elif agent.type == \"Excavator\": agent_id = 1\n",
    "          else: agent_id = 2\n",
    "\n",
    "          if(distance == 0):\n",
    "            continue\n",
    "          agent_density[4*agent_id + quadrant] += 1/distance\n",
    "\n",
    "        return agent_density\n",
    "\n",
    "    def reward(self, closest_dig_site):\n",
    "        return 1/ torch.norm(torch.tensor(self.position, requires_grad=True) - torch.tensor(closest_dig_site.position), p=2).requires_grad_()\n",
    "\n",
    "    def find_closest_dig_site(self, sites):\n",
    "      min = 100000000\n",
    "      best = sites[0]\n",
    "      for i in sites:\n",
    "        dist = np.linalg.norm(self.position-i.position)\n",
    "        if(dist < min):\n",
    "          min = dist\n",
    "          best = i\n",
    "\n",
    "      return best\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "class Drone:\n",
    "    def __init__(self, Game_x, Game_y, model=Drone_Excavator_architecture()):\n",
    "        self.type = 'Drone'\n",
    "        self.position = np.array([np.random.uniform(0,Game_x), np.random.uniform(0,Game_y)])\n",
    "        self.model = model\n",
    "        self.Game_x = Game_x\n",
    "        self.Game_y = Game_y\n",
    "\n",
    "    def compute_quadrant(self, position):\n",
    "      quadrant = 0\n",
    "      if(position[0] <= self.Game_x//2):\n",
    "        if(position[1] <= self.Game_y //2):\n",
    "          quadrant = 1\n",
    "        else:\n",
    "          qaudrant = 3\n",
    "      else:\n",
    "        if(position[1] <= self.Game_y //2):\n",
    "          quadrant = 2\n",
    "        else:\n",
    "          quadrant = 4\n",
    "      return quadrant\n",
    "\n",
    "    def get_next_move(self,sites, agents):\n",
    "      input_data = torch.Tensor(self.compute_site_density(sites)+ self.compute_agent_density(agents))\n",
    "      dx,dy = self.model.forward(input_data)\n",
    "      self.position[0] += dx\n",
    "      self.position[1] += dy\n",
    "      if self.position[0] > self.Game_x: self.position[0] = self.Game_x\n",
    "      if self.position[1] > self.Game_y: self.position[1] = self.Game_y\n",
    "      return [dx,dy]\n",
    "\n",
    "\n",
    "    def rollout_island(self, sites, agents, num_steps):\n",
    "      trajectory = []\n",
    "\n",
    "      for _ in range(num_steps):\n",
    "        # Get the current state observation (site and agent data)\n",
    "        site_density = self.compute_site_density(sites)\n",
    "        agent_density = self.compute_agent_density(agents)\n",
    "        state = torch.Tensor(site_density + agent_density)\n",
    "\n",
    "        # Store the current state\n",
    "        current_state = state.clone()\n",
    "\n",
    "        # Choose an action using the policy (assuming a discrete action space)\n",
    "        action = self.get_next_move(sites,agents)\n",
    "\n",
    "        # Calculate the reward based on the new state\n",
    "        closest_dig_site = self.find_closest_dig_site(sites)\n",
    "        reward = self.reward(closest_dig_site)\n",
    "\n",
    "        # Store the data for this time step\n",
    "        trajectory.append([current_state, action, reward])\n",
    "\n",
    "      return trajectory\n",
    "\n",
    "    def compute_site_density(self, sites):\n",
    "        # Implement equation 3\n",
    "        site_density = [0]*4\n",
    "        for site in sites:\n",
    "            quadrant = site.quadrant-1\n",
    "            distance = np.linalg.norm(self.position - site.position)\n",
    "            if site.type == 'Marked':\n",
    "              site_density[quadrant] += site.value/distance\n",
    "        return site_density\n",
    "\n",
    "    def compute_agent_density(self, agents):\n",
    "        # Implement equation 2 first 4 rover , then excavator, then drone\n",
    "        agent_density = [0]*12\n",
    "        for agent in agents:\n",
    "          distance = np.linalg.norm(self.position - agent.position)\n",
    "          quadrant = self.compute_quadrant(agent.position)-1\n",
    "          if agent.type == \"Rover\": agent_id = 0\n",
    "          elif agent.type == \"Excavator\": agent_id = 1\n",
    "          else: agent_id = 2\n",
    "\n",
    "          if(distance == 0):\n",
    "            continue\n",
    "          agent_density[4*agent_id + quadrant] += 1/distance\n",
    "\n",
    "        return agent_density\n",
    "\n",
    "    def reward(self, closest_dig_site):\n",
    "        return 1/ torch.norm(torch.tensor(self.position, requires_grad=True) - torch.tensor(closest_dig_site.position), p=2).requires_grad_()\n",
    "\n",
    "    def find_closest_dig_site(self, sites):\n",
    "      min = 100000000\n",
    "      best = sites[0]\n",
    "      for i in sites:\n",
    "        dist = np.linalg.norm(self.position-i.position)\n",
    "        if(dist < min):\n",
    "          min = dist\n",
    "          best = i\n",
    "\n",
    "      return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "XRp-IRR-SI0p"
   },
   "outputs": [],
   "source": [
    "class Island:\n",
    "    def __init__(self, a_type, numAgents):\n",
    "        self.agent_type = a_type\n",
    "        self.numAgents = numAgents\n",
    "        self.agents = []\n",
    "        self.dataset = []\n",
    "\n",
    "\n",
    "    def initialize_agents(self,dim):\n",
    "        self.dim = dim\n",
    "        if(self.agent_type == 'Rover'):\n",
    "            for i in range(self.numAgents):\n",
    "                self.agents.append(Rover(dim[0],dim[1]))\n",
    "\n",
    "        if(self.agent_type == 'Excavator'):\n",
    "            for i in range(self.numAgents):\n",
    "                self.agents.append(Excavator(dim[0],dim[1]))\n",
    "\n",
    "        if(self.agent_type == 'Drone'):\n",
    "            for i in range(self.numAgents):\n",
    "                self.agents.append(Drone(dim[0],dim[1]))\n",
    "\n",
    "\n",
    "    def update_policies(self, sites, agents, N):\n",
    "      # Algorithm 1\n",
    "        for i in range(N):\n",
    "            policy = random.choice(self.agents)\n",
    "\n",
    "            # perturb weights\n",
    "            parameters_list = list(policy.model.parameters())\n",
    "            ## Generate Gaussian noise with the same shape as the tensor\n",
    "            noise = torch.randn(parameters_list[2].size()) * 1 + 0.1\n",
    "            for i,param in enumerate(policy.model.parameters()):\n",
    "              if(i ==2):\n",
    "                param.data += noise\n",
    "\n",
    "\n",
    "            # Perform a rollout using the policy\n",
    "            rollout_data = policy.rollout_island(sites,agents,20)  # Implement the 'rollout' method for each agent\n",
    "\n",
    "            # Apply PPO\n",
    "            policy.model = self.train_ppo(policy.model, rollout_data)\n",
    "\n",
    "            # add to population\n",
    "            agents.append(policy)\n",
    "\n",
    "\n",
    "    def train_ppo(self, model, rollout_data):\n",
    "        model.train()\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "        for state, action, reward in rollout_data:\n",
    "          loss = -1 * reward\n",
    "          optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "        model.eval()\n",
    "        return model\n",
    "\n",
    "    def add_agents(self, dim, list_agents):\n",
    "      for i in list_agents:\n",
    "        if self.agent_type==\"Rover\":\n",
    "          self.agents += [ Rover(dim[0], dim[1], i.model) ]\n",
    "        elif self.agent_type==\"Excavator\":\n",
    "          self.agents += [ Excavator(dim[0], dim[1], i.model) ]\n",
    "        else:\n",
    "          self.agents += [ Drone(dim[0], dim[1], i.model) ]\n",
    "\n",
    "\n",
    "    def update_latent_space(self):\n",
    "        # Get all models\n",
    "        models = [i.model for i in self.agents]\n",
    "        # get all weights\n",
    "        weight_vectors = []\n",
    "        for model in models:\n",
    "            weights = []\n",
    "            for param in model.parameters():\n",
    "                weights.append(param.data.view(-1).numpy())\n",
    "            weight_vectors.append(np.concatenate(weights))\n",
    "        # Perform PCA to reduce dimensionality\n",
    "        pca = PCA(n_components=20)\n",
    "        reduced_vectors = pca.fit_transform(weight_vectors)\n",
    "        # get furthest vectors\n",
    "        m = self.numAgents\n",
    "        distances = np.linalg.norm(weight_vectors, axis=1)\n",
    "        furthest_indices = np.argpartition(distances, -m)[-m:]\n",
    "        # update models\n",
    "        for i in range(m):\n",
    "          self.agents[i].model = models[furthest_indices[i]]\n",
    "        self.agents = self.agents[:m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "dv33YI8wYkDL"
   },
   "outputs": [],
   "source": [
    "# re-write\n",
    "class Site:\n",
    "\n",
    "  def __init__(self, position, value,quadrant):\n",
    "    self.position = position\n",
    "    self.value = value\n",
    "    self.type = \"Unmarked\"\n",
    "    self.quadrant = quadrant\n",
    "    self.excavated = False\n",
    "    self.time_marked = None\n",
    "###########################################################################################################################\n",
    "class Team:\n",
    "  def __init__(self,list_agents, game_mode):\n",
    "    self.fitness = None\n",
    "    self.R, self.D, self.E = [], [], []\n",
    "    for i in list_agents:\n",
    "      if i.type==\"Rover\": self.R.append(i)\n",
    "      elif i.type==\"Excavator\": self.E.append(i)\n",
    "      else: self.D.append(i)\n",
    "    self.game_mode = game_mode\n",
    "    # coupling values\n",
    "    self.excavator_coupling_requirement = 3\n",
    "    self.rover_coupling_requirement = 3\n",
    "    self.excavator_observation_radius = 15\n",
    "    self.rover_observation_radius = 10\n",
    "    self.drone_observation_radius = 20\n",
    "    self.num_marked = 0\n",
    "    self.num_excavated = 0\n",
    "    self.num_communicated = 0\n",
    "\n",
    "\n",
    "  def mark_site(self, site, curr_ts):\n",
    "      rovs = 0\n",
    "      for rover in self.R:\n",
    "          if np.linalg.norm(rover.position - site.position) <= self.rover_observation_radius:\n",
    "              rovs += 1\n",
    "      if rovs >= self.rover_coupling_requirement:\n",
    "          site.type = 'Marked'\n",
    "          site.time_marked = curr_ts\n",
    "          self.num_marked += 1\n",
    "\n",
    "  def excavate_site(self, site):\n",
    "      excs = 0\n",
    "      for excavator in self.E:\n",
    "          if np.linalg.norm(excavator.position - site.position) <= self.excavator_observation_radius:\n",
    "              excs += 1\n",
    "      if excs >= self.excavator_coupling_requirement:\n",
    "          site.excavated = True\n",
    "          self.num_excavated += 1\n",
    "\n",
    "\n",
    "  def rollout(self, digsites, num_iterations):\n",
    "    agents = self.R + self.E + self.D\n",
    "    for t in range(num_iterations):\n",
    "      for agent in agents:\n",
    "        agent.get_next_move(digsites,agents)\n",
    "      for site in digsites:\n",
    "        self.mark_site(site, t)\n",
    "        if(site.type == \"Marked\"):\n",
    "          self.excavate_site(site)\n",
    "      if self.game_mode is None: pass\n",
    "      elif self.game_mode==\"D\": Decay(digsites)\n",
    "      elif self.game_mode==\"V\": Volatile(digsites, t)\n",
    "      else:\n",
    "        Decay(digsites)\n",
    "        Volatile(digsites,t)\n",
    "\n",
    "  def calc_fitness(self,digsites_updated):\n",
    "    fitness = 0\n",
    "    for dig_site in digsites_updated:\n",
    "        # Implement equation 4\n",
    "\n",
    "      if(dig_site.excavated == False):\n",
    "        continue\n",
    "      drones_covered = 0  # Number of drones covering the dig site\n",
    "      for drone in self.D:\n",
    "          distance = np.linalg.norm(drone.position - dig_site.position)\n",
    "          if distance <= self.drone_observation_radius:\n",
    "              drones_covered += 1\n",
    "\n",
    "      if drones_covered > 0:\n",
    "          fitness += dig_site.value\n",
    "          self.num_communicated += 1\n",
    "\n",
    "\n",
    "    self.fitness = fitness\n",
    "\n",
    "  def get_fitness(self, digsites):\n",
    "    digsites_updated = copy.deepcopy(digsites)\n",
    "    if self.fitness is not None:\n",
    "      return\n",
    "    else:\n",
    "      self.rollout(digsites_updated,20)\n",
    "      self.calc_fitness(digsites_updated)\n",
    "\n",
    "#############################################################################################################################################\n",
    "\n",
    "class Mainland:\n",
    "\n",
    "  def __init__(self, pop_size, team_size, elite_size, X_max, Y_max, num_digsites,game_mode):\n",
    "    self.ts = team_size\n",
    "    self.ps = pop_size\n",
    "    self.dim = [X_max, Y_max]\n",
    "    self.dss = num_digsites\n",
    "    self.es = elite_size\n",
    "    self.sites = []\n",
    "    self.population = []\n",
    "    self.rover_score = 0\n",
    "    self.excavator_score = 0\n",
    "    self.drone_score = 0\n",
    "    self.game_mode = game_mode # D for decay, V for volatile, M\n",
    "\n",
    "  def place_digsites(self):\n",
    "    for _ in range(self.dss):\n",
    "      position = [random.randint(0,self.dim[0]), random.randint(0,self.dim[1])]\n",
    "      value = np.random.randint(1,10)\n",
    "      # find quad\n",
    "      x, y = self.dim\n",
    "      if(position[0] <= x//2):\n",
    "        if(position[1] <= y //2):\n",
    "            quadrant = 1\n",
    "        else:\n",
    "          quadrant = 3\n",
    "      else:\n",
    "          if(position[1] <= y //2):\n",
    "            quadrant = 2\n",
    "          else:\n",
    "            quadrant = 4\n",
    "      self.sites.append(Site(position, value, quadrant))\n",
    "\n",
    "  def update_teams(self, All_r, All_e, All_d):\n",
    "    # input is list of policies of rovers, excavators, drones\n",
    "    All_r = [Rover(self.dim[0], self.dim[1], i) for i in All_r]\n",
    "    All_e = [Excavator(self.dim[0], self.dim[1], i) for i in All_e]\n",
    "    All_d = [Drone(self.dim[0], self.dim[1], i) for i in All_d]\n",
    "    All_agents = copy.deepcopy(All_r + All_e + All_d)\n",
    "    teams = [Team(random.choices(All_agents, k=self.ts), self.game_mode) for _ in range(self.ps)]\n",
    "    self.population = teams\n",
    "\n",
    "  def initialize(self):\n",
    "    # input is list of policies of rovers, excavators, drones\n",
    "    self.place_digsites()\n",
    "\n",
    "  def crossover_possible(self,T1, T2):\n",
    "    # atleast one agent class in common\n",
    "    ret = ((len(T1.R)>0) and (len(T2.R)>0)) or ((len(T1.E)>0) and (len(T2.E)>0)) or ((len(T1.D)>0) and (len(T2.D)>0))\n",
    "    return ret\n",
    "\n",
    "  def crossover(self, T_elite, T_nonelite):\n",
    "    # cross over T_e and T_ne\n",
    "    ## from T_ne, and 2 children return the one with the highest fitness [tournament selection]\n",
    "    while True:\n",
    "      rng = random.randint(1,3)\n",
    "      if rng==1:\n",
    "        # exchange rover\n",
    "        if not ((len(T_elite.R)>0) and (len(T_nonelite.R)>0)):\n",
    "          continue\n",
    "        C1, C2 = copy.deepcopy(T_elite), copy.deepcopy(T_nonelite)\n",
    "        idx1, idx2 = random.choice(range(len(C1.R))), random.choice(range(len(C2.R)))\n",
    "        C1.R[idx1], C2.R[idx2] = C2.R[idx2], C1.R[idx1]\n",
    "      if rng==2:\n",
    "        # exchange excavator\n",
    "        if not ((len(T_elite.E)>0) and (len(T_nonelite.E)>0)):\n",
    "          continue\n",
    "        C1, C2 = copy.deepcopy(T_elite), copy.deepcopy(T_nonelite)\n",
    "        idx1, idx2 = random.choice(range(len(C1.E))), random.choice(range(len(C2.E)))\n",
    "        C1.E[idx1], C2.E[idx2] = C2.E[idx2], C1.E[idx1]\n",
    "      if rng==3:\n",
    "        # exchange drone\n",
    "        if not ((len(T_elite.D)>0) and (len(T_nonelite.D)>0)):\n",
    "          continue\n",
    "        C1, C2 = copy.deepcopy(T_elite), copy.deepcopy(T_nonelite)\n",
    "        idx1, idx2 = random.choice(range(len(C1.D))), random.choice(range(len(C2.D)))\n",
    "        C1.D[idx1], C2.D[idx2] = C2.D[idx2], C1.D[idx1]\n",
    "      break\n",
    "    C1.fitness, C2.fitness = None, None\n",
    "    C1.get_fitness(self.sites)\n",
    "    C2.get_fitness(self.sites)\n",
    "    # parallel for children\n",
    "    with multiprocessing.Pool() as pool:\n",
    "        pop = pool.map(self.parallel, [C1,C2])\n",
    "    l = [T_nonelite, *pop]\n",
    "    l.sort(key=lambda team: team.fitness, reverse=True)\n",
    "    return l[0]\n",
    "\n",
    "  def parallel(self,team_in):\n",
    "    team_in.get_fitness(self.sites)\n",
    "    return team_in\n",
    "\n",
    "  def run_algo(self, N):\n",
    "    # algorithm 2\n",
    "    for gen in tqdm(range(N)):\n",
    "\n",
    "      # get fitness\n",
    "      # for team in self.population:\n",
    "      #   # team.get_fitness(self.sites)\n",
    "      #   self.rover_score = team.num_marked\n",
    "      #   self.excavator_score = team.num_excavated\n",
    "      #   self.drone_score = team.num_communicated\n",
    "      with multiprocessing.Pool() as pool:\n",
    "        pop = pool.map(self.parallel, self.population)\n",
    "      self.population = pop\n",
    "    \n",
    "      # get the set E\n",
    "      self.population.sort(key=lambda team: team.fitness, reverse=True)\n",
    "      E = self.population[:self.es]\n",
    "      T_e = self.population[self.es+1:]\n",
    "\n",
    "      # cross over\n",
    "      S = []\n",
    "      for pi_y in T_e:\n",
    "        pi_x = random.choice(E)\n",
    "        while not self.crossover_possible(pi_x, pi_y):\n",
    "          pi_x = random.choice(E)\n",
    "        S.append(self.crossover(pi_x, pi_y))\n",
    "\n",
    "      # T <- S ∪ E\n",
    "      self.population = E + S\n",
    "        \n",
    "    for team in self.population:\n",
    "      self.rover_score += team.num_marked\n",
    "      self.excavator_score += team.num_excavated\n",
    "      self.drone_score += team.num_communicated\n",
    "\n",
    "  def get_elite(self):\n",
    "    for team in self.population: team.get_fitness(self.sites)\n",
    "    self.population.sort(key=lambda team: team.fitness, reverse=True)\n",
    "    E = self.population[:self.es]\n",
    "    return E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "RrhKRsM_1I2e"
   },
   "outputs": [],
   "source": [
    "def init_weights(num_mainlands, num_islands):\n",
    "  global weights\n",
    "  weights = np.zeros((num_mainlands,num_islands))\n",
    "  prob = np.zeros((num_mainlands, num_islands))\n",
    "  for i in range(num_mainlands):\n",
    "    for j in range(num_islands):\n",
    "      weights[i][j] = 1/num_islands ## initially start with every island having equal weightage\n",
    "\n",
    "#############################################################################################################################################\n",
    "def distribution_from_weights(num_mainlands,num_islands):\n",
    "  global prob\n",
    "  prob = np.zeros((num_mainlands, num_islands))\n",
    "  for i in range(num_mainlands):\n",
    "    for j in range(num_islands):\n",
    "      prob[i][j] = math.exp(weights[i][j])/np.sum(np.exp(weights[:, j]))\n",
    "#############################################################################################################################################\n",
    "def get_gradient_matrix(i,j):\n",
    "  global weights\n",
    "  weight_tensor = torch.tensor(weights.copy(), requires_grad=True)\n",
    "  prob_tensor = torch.exp(weight_tensor) / torch.sum(torch.exp(weight_tensor), dim=0)\n",
    "  target_probability = prob_tensor[i][j]\n",
    "\n",
    "  # Compute the gradient using automatic differentiation\n",
    "  target_probability.backward()\n",
    "  # The gradient is now stored in weights.grad\n",
    "  gradient = weight_tensor.grad\n",
    "  gradient_matrix = gradient.numpy()\n",
    "  gradient_vector = gradient_matrix[:,j]\n",
    "  return gradient_vector\n",
    "#############################################################################################################################################\n",
    "def update_weights(Mainlands, num_islands, alpha=0.1, nu=0.01):\n",
    "    global weights\n",
    "    num_mainlands = len(Mainlands)\n",
    "    for j in range(num_islands):\n",
    "        drone_score = []\n",
    "        update = np.zeros(num_mainlands)\n",
    "        for m in range(1, num_mainlands + 1):\n",
    "            temp = []\n",
    "            gradient = get_gradient_matrix(m-1,j)  # ∇𝑤𝜇(𝑚,𝑖)\n",
    "            if(j == 0 ):\n",
    "              performance = Mainlands[m-1].rover_score  # 𝑓𝑚,𝑖\n",
    "            elif(j == 1):\n",
    "              performance = Mainlands[m-1].excavator_score  # 𝑓𝑚,𝑖\n",
    "            else:\n",
    "              temp.append(Mainlands[m-1].drone_score)\n",
    "              performance = Mainlands[m-1].drone_score  # 𝑓𝑚,𝑖\n",
    "            log_term = math.log(prob[m - 1][j])  # 𝜈𝑙𝑜𝑔𝜇(𝑚,𝑖)\n",
    "            update += gradient * (performance - nu * log_term)\n",
    "            drone_score.append(temp)\n",
    "        # print(f\"performance {performance}\")\n",
    "        # print(f\"log_term {log_term}\")\n",
    "        # print(f\"update {update}\")\n",
    "        \n",
    "\n",
    "        weights[:,j] += alpha * update  # Apply the update\n",
    "    global store_weights, sites_interacted\n",
    "    store_weights.append(copy.deepcopy(weights))\n",
    "    sites_interacted.append(drone_score)\n",
    "#############################################################################################################################################\n",
    "def select_agents(num_mainlands, Islands):\n",
    "  net_distribution = []\n",
    "  num_islands = len(Islands)\n",
    "  for m in range(num_mainlands):\n",
    "    for i in range(num_islands):\n",
    "      if Islands[i].agent_type == \"Rover\":\n",
    "          rovers = random.choices(Islands[i].agents, k=int(len(Islands[i].agents)*prob[m][i]))\n",
    "      elif Islands[i].agent_type == \"Excavator\":\n",
    "          excavators = random.choices(Islands[i].agents, k=int(len(Islands[i].agents)*prob[m][i]))\n",
    "      else:\n",
    "          drones = random.choices(Islands[i].agents, k=int(len(Islands[i].agents)*prob[m][i]))\n",
    "\n",
    "    net_distribution.append([[k.model for k in rovers],[k.model for k in excavators],[k.model for k in drones]])\n",
    "\n",
    "  return net_distribution\n",
    "#############################################################################################################################################\n",
    "# Decay and Volatile\n",
    "def Decay(site_list):\n",
    "  for site in site_list:\n",
    "    if site.type==\"Unmarked\":\n",
    "      site.value *= 0.5\n",
    "def Volatile(site_list, curr_ts):\n",
    "  for site in site_list:\n",
    "    if (site.type==\"Marked\") and (site.excavated==False):\n",
    "      if curr_ts - site.time_marked > 7:\n",
    "        site.type = \"Unmarked\"\n",
    "        site.time_marked = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1GuzLv45He35",
    "outputId": "f009f8a8-0a44-45c7-ce36-d1e3dce26ead"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5 0.5 0.5]\n",
      " [0.5 0.5 0.5]]\n"
     ]
    }
   ],
   "source": [
    "# rollout and rollout_island has numIter hyperparameter [hardcoded]\n",
    "num_iterations = 12\n",
    "num_agents_per_island = 150\n",
    "num_mainlands = 2\n",
    "team_size = 10\n",
    "pop_size = 50\n",
    "elite_size = 10\n",
    "mainland_mode = None\n",
    "Island_m = Mainland(0,0,0,60, 60,15, None)\n",
    "Island_m.initialize()\n",
    "init_weights(num_mainlands,3)\n",
    "distribution_from_weights(num_mainlands,3)\n",
    "print(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_weights = []\n",
    "sites_interacted = []\n",
    "highest_team_fitness = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qSKl-dX70LYL",
    "outputId": "58d5edc8-0b01-4792-aabe-a578a6398323",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## This cell is for the policy migration code, which has 4 parts,\n",
    "## 1) defining a softmax function to get the distribution of agents\n",
    "## 2) Invoking N iterations of islands and mainlands\n",
    "## 3) Sending elite teams back to the islands\n",
    "## 4) Updating the softmax with a gradient\n",
    "\n",
    "# Algorithm 3\n",
    "## Initialize islands\n",
    "Island_list = [Island(\"Rover\", num_agents_per_island), Island(\"Excavator\", num_agents_per_island), Island(\"Drone\", num_agents_per_island)]\n",
    "for Is in Island_list:\n",
    "  Is.initialize_agents(Island_m.dim)\n",
    "\n",
    "## Initialize Mainlands\n",
    "# Mainland_list = [Mainland(pop_size,team_size,elite_size, 60,60, np.random.randint(1,20), mainland_mode) for _ in range(num_mainlands)]\n",
    "Mainland_list = [Mainland(pop_size,team_size,elite_size, 60,60,20,\"D\"),Mainland(pop_size,team_size,elite_size, 60,60,20,None)]\n",
    "for M in Mainland_list:\n",
    "  M.initialize()\n",
    "\n",
    "for k in range(num_iterations):\n",
    "\n",
    "  # 𝑃𝑜𝑝_𝐼 = islands(𝑃𝑜𝑝_𝐼)\n",
    "  for Is in Island_list:\n",
    "    Is.update_policies(Island_m.sites,Is.agents, 50)\n",
    "    Is.update_latent_space()\n",
    "  # 𝑇_𝑀 = mainlands(𝑇_𝑀)\n",
    "  for m, M in enumerate(Mainland_list):\n",
    "    print(m,end=\"\")\n",
    "    agent_distribution = select_agents(num_mainlands,Island_list)\n",
    "    rovers = agent_distribution[m][0]\n",
    "    excavators = agent_distribution[m][1]\n",
    "    drones = agent_distribution[m][2]\n",
    "    M.update_teams(rovers, excavators,drones)\n",
    "    M.run_algo(7)\n",
    "  print()\n",
    "\n",
    "  # 𝑃𝑜𝑝_𝑖 ← 𝑃𝑜𝑝_𝑖 ∪ 𝑇_(𝑚,𝑖)[0:𝑒] ∀𝑚 ∈ M\n",
    "  highest_team_fitness.append([-1*np.inf for i in range(num_mainlands)])\n",
    "  for m, M in enumerate(Mainland_list):\n",
    "    E = M.get_elite()\n",
    "    for elite in E:\n",
    "      Island_list[0].add_agents(Island_m.dim,elite.R)\n",
    "      Island_list[1].add_agents(Island_m.dim, elite.E)\n",
    "      Island_list[2].add_agents(Island_m.dim, elite.D)\n",
    "      if elite.fitness > highest_team_fitness[-1][m]:\n",
    "            highest_team_fitness[-1][m] = elite.fitness\n",
    "  for Is in Island_list:\n",
    "    Is.update_latent_space()\n",
    "  # 𝑤_[𝑘+1,𝑖] ← update(𝑤_[𝑘,𝑖])\n",
    "  update_weights(Mainland_list,3)\n",
    "\n",
    "  for m, M in enumerate(Mainland_list):\n",
    "    # /* Replace ( |𝑇 | − 𝑒 ) teams by sampling islands */\n",
    "    # 13 𝑇_𝑚 ← 𝑇_𝑚[0:𝑒]∪(|𝑇|−𝑒) ∼ 𝑤_(𝑘+1,𝑖), ∀𝑖 ∈ I\n",
    "    Tm_0_e = M.get_elite()\n",
    "    # add new teams from islands\n",
    "    agent_distribution = select_agents(num_mainlands,Island_list)\n",
    "    rovers = agent_distribution[m][0]\n",
    "    excavators = agent_distribution[m][1]\n",
    "    drones = agent_distribution[m][2]\n",
    "    M.update_teams(rovers, excavators,drones)\n",
    "    M.population = M.population[:-1*len(E)]\n",
    "    # add elite teams back\n",
    "    M.population += Tm_0_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.44166667,  0.33333333,  0.33333333],\n",
       "        [ 1.10833333,  0.33333333,  0.33333333]]),\n",
       " array([[ 2.49028399,  0.33333333,  0.33333333],\n",
       "        [-1.82361732,  0.33333333,  0.33333333]]),\n",
       " array([[ 2.85122025,  0.33333333,  0.33333333],\n",
       "        [-2.18455358,  0.33333333,  0.33333333]]),\n",
       " array([[ 3.06363792,  0.33333333,  0.33333333],\n",
       "        [-2.39697125,  0.33333333,  0.33333333]]),\n",
       " array([[ 3.29546584,  0.33333333,  0.33333333],\n",
       "        [-2.62879917,  0.33333333,  0.33333333]]),\n",
       " array([[ 3.51434526,  0.33333333,  0.33333333],\n",
       "        [-2.84767859,  0.33333333,  0.33333333]]),\n",
       " array([[ 3.69751763,  0.33333333,  0.33333333],\n",
       "        [-3.03085097,  0.33333333,  0.33333333]]),\n",
       " array([[ 3.88002294,  0.33333333,  0.33333333],\n",
       "        [-3.21335627,  0.33333333,  0.33333333]]),\n",
       " array([[ 4.0318509 ,  0.33333333,  0.33333333],\n",
       "        [-3.36518423,  0.33333333,  0.33333333]]),\n",
       " array([[ 4.15676351,  0.33333333,  0.33333333],\n",
       "        [-3.49009684,  0.33333333,  0.33333333]]),\n",
       " array([[ 4.25676059,  2.83333333,  0.45833333],\n",
       "        [-3.59009393, -2.16666667,  0.20833333]]),\n",
       " array([[ 4.34470138,  2.8998139 ,  0.58140037],\n",
       "        [-3.67803471, -2.23314723,  0.08526629]])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
