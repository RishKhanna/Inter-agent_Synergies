{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKOl39Vt24Zo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy as sp\n",
        "import math, copy, random, multiprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from sklearn.decomposition import PCA\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WBtZy1vpNTH"
      },
      "outputs": [],
      "source": [
        "class Rover_architecture(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Rover_architecture, self).__init__()\n",
        "        self.fc1 = nn.Linear(20, 32)  # Input layer (16 units to 32 units)\n",
        "        self.fc2 = nn.Linear(32, 32)  # Hidden layer (32 units to 32 units)\n",
        "        self.fc3 = nn.Linear(32, 2)   # Output layer (32 units to 2 units)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.tanh(self.fc1(x))\n",
        "        x = self.tanh(self.fc2(x))\n",
        "        x = self.tanh(self.fc3(x))\n",
        "        return x\n",
        "\n",
        "class Drone_Excavator_architecture(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Drone_Excavator_architecture, self).__init__()\n",
        "        self.fc1 = nn.Linear(16, 32)  # Input layer (16 units to 32 units)\n",
        "        self.fc2 = nn.Linear(32, 32)  # Hidden layer (32 units to 32 units)\n",
        "        self.fc3 = nn.Linear(32, 2)   # Output layer (32 units to 2 units)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.tanh(self.fc1(x))\n",
        "        x = self.tanh(self.fc2(x))\n",
        "        x = self.tanh(self.fc3(x))\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwEwFn-g25ol"
      },
      "outputs": [],
      "source": [
        "class Rover:\n",
        "    def __init__(self, Game_x, Game_y, model=Rover_architecture()):\n",
        "        self.type = 'Rover'\n",
        "        self.position = np.array([np.random.uniform(0,Game_x), np.random.uniform(0,Game_y)])\n",
        "        self.model = model\n",
        "        self.Game_x = Game_x\n",
        "        self.Game_y = Game_y\n",
        "\n",
        "    def compute_quadrant(self, position):\n",
        "      quadrant = 0\n",
        "      if(position[0] <= self.Game_x//2):\n",
        "        if(position[1] <= self.Game_y //2):\n",
        "          quadrant = 1\n",
        "        else:\n",
        "          qaudrant = 3\n",
        "      else:\n",
        "        if(position[1] <= self.Game_y //2):\n",
        "          quadrant = 2\n",
        "        else:\n",
        "          quadrant = 4\n",
        "      return quadrant\n",
        "\n",
        "    def get_next_move(self,sites, agents):\n",
        "      input_data = torch.Tensor(self.compute_site_density(sites)+ self.compute_agent_density(agents))\n",
        "      dx,dy = self.model.forward(input_data)\n",
        "      self.position[0] += dx\n",
        "      self.position[1] += dy\n",
        "      return [dx,dy]\n",
        "\n",
        "\n",
        "    def rollout_island(self, sites, agents, num_steps):\n",
        "      trajectory = []\n",
        "\n",
        "      for _ in range(num_steps):\n",
        "        # Get the current state observation (site and agent data)\n",
        "        site_density = self.compute_site_density(sites)\n",
        "        agent_density = self.compute_agent_density(agents)\n",
        "        state = torch.Tensor(site_density + agent_density)\n",
        "\n",
        "        # Store the current state\n",
        "        current_state = state.clone()\n",
        "\n",
        "        # Choose an action using the policy (assuming a discrete action space)\n",
        "        action = self.get_next_move(sites,agents)\n",
        "\n",
        "\n",
        "        # Calculate the reward based on the new state\n",
        "        closest_dig_site = self.find_closest_dig_site(sites)\n",
        "        reward = self.reward(closest_dig_site)\n",
        "\n",
        "        # Store the data for this time step\n",
        "        trajectory.append([current_state, action, reward])\n",
        "\n",
        "      return trajectory\n",
        "\n",
        "    def compute_site_density(self, sites):\n",
        "        # Implement equation 3\n",
        "        site_density = [0]*8\n",
        "        for site in sites:\n",
        "            quadrant = site.quadrant-1\n",
        "            distance = np.linalg.norm(self.position - site.position)\n",
        "            if site.type == 'Marked':\n",
        "              site_density[quadrant] += site.value/distance\n",
        "            if site.type == 'Unmarked':\n",
        "              site_density[quadrant+4] += site.value / distance\n",
        "        return site_density\n",
        "\n",
        "    def compute_agent_density(self, agents):\n",
        "        # Implement equation 2 first 4 rover , then excavator, then drone\n",
        "        agent_density = [0]*12\n",
        "        for agent in agents:\n",
        "          distance = np.linalg.norm(self.position - agent.position)\n",
        "          quadrant = self.compute_quadrant(agent.position)-1\n",
        "          if agent.type == \"Rover\": agent_id = 0\n",
        "          elif agent.type == \"Excavator\": agent_id = 1\n",
        "          else: agent_id = 2\n",
        "\n",
        "          if(distance == 0):\n",
        "            continue\n",
        "          agent_density[4*agent_id + quadrant] += 1/distance\n",
        "\n",
        "        return agent_density\n",
        "\n",
        "    def reward(self, closest_dig_site):\n",
        "        return 1/ torch.norm(torch.tensor(self.position, requires_grad=True) - torch.tensor(closest_dig_site.position), p=2).requires_grad_()\n",
        "\n",
        "    def find_closest_dig_site(self, sites):\n",
        "      min = 100000000\n",
        "      best = sites[0]\n",
        "      for i in sites:\n",
        "        dist = np.linalg.norm(self.position-i.position)\n",
        "        if(dist < min):\n",
        "          min = dist\n",
        "          best = i\n",
        "\n",
        "      return best\n",
        "\n",
        "########################################################################################################################\n",
        "\n",
        "class Excavator:\n",
        "    def __init__(self, Game_x, Game_y, model=Drone_Excavator_architecture()):\n",
        "        self.type = 'Excavator'\n",
        "        self.position = np.array([np.random.uniform(0,Game_x), np.random.uniform(0,Game_y)])\n",
        "        self.model = model\n",
        "        self.Game_x = Game_x\n",
        "        self.Game_y = Game_y\n",
        "\n",
        "    def compute_quadrant(self, position):\n",
        "      quadrant = 0\n",
        "\n",
        "      if(position[0] <= self.Game_x//2):\n",
        "        if(position[1] <= self.Game_y //2):\n",
        "          quadrant = 1\n",
        "        else:\n",
        "          qaudrant = 3\n",
        "      else:\n",
        "        if(position[1] <= self.Game_y //2):\n",
        "          quadrant = 2\n",
        "        else:\n",
        "          quadrant = 4\n",
        "      return quadrant\n",
        "\n",
        "    def get_next_move(self,sites, agents):\n",
        "      input_data = torch.Tensor(self.compute_site_density(sites)+ self.compute_agent_density(agents))\n",
        "      dx,dy = self.model.forward(input_data)\n",
        "      self.position[0] += dx\n",
        "      self.position[1] += dy\n",
        "      return [dx,dy]\n",
        "\n",
        "\n",
        "    def rollout_island(self, sites, agents, num_steps):\n",
        "      trajectory = []\n",
        "\n",
        "      for _ in range(num_steps):\n",
        "        # Get the current state observation (site and agent data)\n",
        "        site_density = self.compute_site_density(sites)\n",
        "        agent_density = self.compute_agent_density(agents)\n",
        "        state = torch.Tensor(site_density + agent_density)\n",
        "\n",
        "        # Store the current state\n",
        "        current_state = state.clone()\n",
        "\n",
        "        # Choose an action using the policy (assuming a discrete action space)\n",
        "        action = self.get_next_move(sites,agents)\n",
        "\n",
        "\n",
        "        # Calculate the reward based on the new state\n",
        "        closest_dig_site = self.find_closest_dig_site(sites)\n",
        "        reward = self.reward(closest_dig_site)\n",
        "\n",
        "        # Store the data for this time step\n",
        "        trajectory.append([current_state, action, reward])\n",
        "\n",
        "      return trajectory\n",
        "\n",
        "    def compute_site_density(self, sites):\n",
        "        # Implement equation 3\n",
        "        site_density = [0]*4\n",
        "        for site in sites:\n",
        "            quadrant = site.quadrant-1\n",
        "            distance = np.linalg.norm(self.position - site.position)\n",
        "            if site.type == 'Marked':\n",
        "              site_density[quadrant] += site.value/distance\n",
        "        return site_density\n",
        "\n",
        "    def compute_agent_density(self, agents):\n",
        "        # Implement equation 2 first 4 rover , then excavator, then drone\n",
        "        agent_density = [0]*12\n",
        "        for agent in agents:\n",
        "          distance = np.linalg.norm(self.position - agent.position)\n",
        "          quadrant = self.compute_quadrant(agent.position)-1\n",
        "          if agent.type == \"Rover\": agent_id = 0\n",
        "          elif agent.type == \"Excavator\": agent_id = 1\n",
        "          else: agent_id = 2\n",
        "\n",
        "          if(distance == 0):\n",
        "            continue\n",
        "          agent_density[4*agent_id + quadrant] += 1/distance\n",
        "\n",
        "        return agent_density\n",
        "\n",
        "    def reward(self, closest_dig_site):\n",
        "        return 1/ torch.norm(torch.tensor(self.position, requires_grad=True) - torch.tensor(closest_dig_site.position), p=2).requires_grad_()\n",
        "\n",
        "    def find_closest_dig_site(self, sites):\n",
        "      min = 100000000\n",
        "      best = sites[0]\n",
        "      for i in sites:\n",
        "        dist = np.linalg.norm(self.position-i.position)\n",
        "        if(dist < min):\n",
        "          min = dist\n",
        "          best = i\n",
        "\n",
        "      return best\n",
        "\n",
        "########################################################################################################################\n",
        "\n",
        "class Drone:\n",
        "    def __init__(self, Game_x, Game_y, model=Drone_Excavator_architecture()):\n",
        "        self.type = 'Drone'\n",
        "        self.position = np.array([np.random.uniform(0,Game_x), np.random.uniform(0,Game_y)])\n",
        "        self.model = model\n",
        "        self.Game_x = Game_x\n",
        "        self.Game_y = Game_y\n",
        "\n",
        "    def compute_quadrant(self, position):\n",
        "      quadrant = 0\n",
        "      if(position[0] <= self.Game_x//2):\n",
        "        if(position[1] <= self.Game_y //2):\n",
        "          quadrant = 1\n",
        "        else:\n",
        "          qaudrant = 3\n",
        "      else:\n",
        "        if(position[1] <= self.Game_y //2):\n",
        "          quadrant = 2\n",
        "        else:\n",
        "          quadrant = 4\n",
        "      return quadrant\n",
        "\n",
        "    def get_next_move(self,sites, agents):\n",
        "      input_data = torch.Tensor(self.compute_site_density(sites)+ self.compute_agent_density(agents))\n",
        "      dx,dy = self.model.forward(input_data)\n",
        "      self.position[0] += dx\n",
        "      self.position[1] += dy\n",
        "      return [dx,dy]\n",
        "\n",
        "\n",
        "    def rollout_island(self, sites, agents, num_steps):\n",
        "      trajectory = []\n",
        "\n",
        "      for _ in range(num_steps):\n",
        "        # Get the current state observation (site and agent data)\n",
        "        site_density = self.compute_site_density(sites)\n",
        "        agent_density = self.compute_agent_density(agents)\n",
        "        state = torch.Tensor(site_density + agent_density)\n",
        "\n",
        "        # Store the current state\n",
        "        current_state = state.clone()\n",
        "\n",
        "        # Choose an action using the policy (assuming a discrete action space)\n",
        "        action = self.get_next_move(sites,agents)\n",
        "\n",
        "        # Calculate the reward based on the new state\n",
        "        closest_dig_site = self.find_closest_dig_site(sites)\n",
        "        reward = self.reward(closest_dig_site)\n",
        "\n",
        "        # Store the data for this time step\n",
        "        trajectory.append([current_state, action, reward])\n",
        "\n",
        "      return trajectory\n",
        "\n",
        "    def compute_site_density(self, sites):\n",
        "        # Implement equation 3\n",
        "        site_density = [0]*4\n",
        "        for site in sites:\n",
        "            quadrant = site.quadrant-1\n",
        "            distance = np.linalg.norm(self.position - site.position)\n",
        "            if site.type == 'Marked':\n",
        "              site_density[quadrant] += site.value/distance\n",
        "        return site_density\n",
        "\n",
        "    def compute_agent_density(self, agents):\n",
        "        # Implement equation 2 first 4 rover , then excavator, then drone\n",
        "        agent_density = [0]*12\n",
        "        for agent in agents:\n",
        "          distance = np.linalg.norm(self.position - agent.position)\n",
        "          quadrant = self.compute_quadrant(agent.position)-1\n",
        "          if agent.type == \"Rover\": agent_id = 0\n",
        "          elif agent.type == \"Excavator\": agent_id = 1\n",
        "          else: agent_id = 2\n",
        "\n",
        "          if(distance == 0):\n",
        "            continue\n",
        "          agent_density[4*agent_id + quadrant] += 1/distance\n",
        "\n",
        "        return agent_density\n",
        "\n",
        "    def reward(self, closest_dig_site):\n",
        "        return 1/ torch.norm(torch.tensor(self.position, requires_grad=True) - torch.tensor(closest_dig_site.position), p=2).requires_grad_()\n",
        "\n",
        "    def find_closest_dig_site(self, sites):\n",
        "      min = 100000000\n",
        "      best = sites[0]\n",
        "      for i in sites:\n",
        "        dist = np.linalg.norm(self.position-i.position)\n",
        "        if(dist < min):\n",
        "          min = dist\n",
        "          best = i\n",
        "\n",
        "      return best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRp-IRR-SI0p"
      },
      "outputs": [],
      "source": [
        "class Island:\n",
        "    def __init__(self, a_type, numAgents):\n",
        "        self.agent_type = a_type\n",
        "        self.numAgents = numAgents\n",
        "        self.agents = []\n",
        "        self.dataset = []\n",
        "\n",
        "\n",
        "    def initialize_agents(self,dim):\n",
        "        self.dim = dim\n",
        "        if(self.agent_type == 'Rover'):\n",
        "            for i in range(self.numAgents):\n",
        "                self.agents.append(Rover(dim[0],dim[1]))\n",
        "\n",
        "        if(self.agent_type == 'Excavator'):\n",
        "            for i in range(self.numAgents):\n",
        "                self.agents.append(Excavator(dim[0],dim[1]))\n",
        "\n",
        "        if(self.agent_type == 'Drone'):\n",
        "            for i in range(self.numAgents):\n",
        "                self.agents.append(Drone(dim[0],dim[1]))\n",
        "\n",
        "\n",
        "    def update_policies(self, sites, agents, N):\n",
        "      # Algorithm 1\n",
        "        for i in range(N):\n",
        "            policy = random.choice(self.agents)\n",
        "\n",
        "            # perturb weights\n",
        "            parameters_list = list(policy.model.parameters())\n",
        "            ## Generate Gaussian noise with the same shape as the tensor\n",
        "            noise = torch.randn(parameters_list[2].size()) * 1 + 0.1\n",
        "            for i,param in enumerate(policy.model.parameters()):\n",
        "              if(i ==2):\n",
        "                param.data += noise\n",
        "\n",
        "\n",
        "            # Perform a rollout using the policy\n",
        "            rollout_data = policy.rollout_island(sites,agents,50)  # Implement the 'rollout' method for each agent\n",
        "\n",
        "            # Apply PPO\n",
        "            policy.model = self.train_ppo(policy.model, rollout_data)\n",
        "\n",
        "            # add to population\n",
        "            agents.append(policy)\n",
        "\n",
        "\n",
        "    def train_ppo(self, model, rollout_data):\n",
        "        model.train()\n",
        "        optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "        for state, action, reward in rollout_data:\n",
        "          loss = -1 * reward\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "        model.eval()\n",
        "        return model\n",
        "\n",
        "    def add_agents(self,dim, list_agents):\n",
        "      for i in list_agents:\n",
        "        if self.agent_type==\"Rover\":\n",
        "          self.agents += [ Rover(dim[0], dim[1], i.model) ]\n",
        "        elif self.agent_type==\"Excavator\":\n",
        "          self.agents += [ Excavator(dim[0], dim[1], i.model) ]\n",
        "        else:\n",
        "          self.agents += [ Drone(dim[0], dim[1], i.model) ]\n",
        "\n",
        "\n",
        "    def update_latent_space(self):\n",
        "        # Get all models\n",
        "        models = [i.model for i in self.agents]\n",
        "        # get all weights\n",
        "        weight_vectors = []\n",
        "        for model in models:\n",
        "            weights = []\n",
        "            for param in model.parameters():\n",
        "                weights.append(param.data.view(-1).numpy())\n",
        "            weight_vectors.append(np.concatenate(weights))\n",
        "        # Perform PCA to reduce dimensionality\n",
        "        pca = PCA(n_components=20)\n",
        "        reduced_vectors = pca.fit_transform(weight_vectors)\n",
        "        # get furthest vectors\n",
        "        m = self.numAgents\n",
        "        distances = np.linalg.norm(weight_vectors, axis=1)\n",
        "        furthest_indices = np.argpartition(distances, -m)[-m:]\n",
        "        # update models\n",
        "        for i in range(m):\n",
        "          self.agents[i].model = models[furthest_indices[i]]\n",
        "        self.agents = self.agents[:m]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dv33YI8wYkDL"
      },
      "outputs": [],
      "source": [
        "# re-write\n",
        "class Site:\n",
        "\n",
        "  def __init__(self, position, value,quadrant):\n",
        "    self.position = position\n",
        "    self.value = value\n",
        "    self.type = \"Unmarked\"\n",
        "    self.quadrant = quadrant\n",
        "    self.excavated = False\n",
        "    self.time_marked = None\n",
        "###########################################################################################################################\n",
        "class Team:\n",
        "  def __init__(self,list_agents, game_mode):\n",
        "    self.fitness = None\n",
        "    self.R, self.D, self.E = [], [], []\n",
        "    for i in list_agents:\n",
        "      if i.type==\"Rover\": self.R.append(i)\n",
        "      elif i.type==\"Excavator\": self.E.append(i)\n",
        "      else: self.D.append(i)\n",
        "    self.game_mode = game_mode\n",
        "    # coupling values\n",
        "    self.excavator_coupling_requirement = 3\n",
        "    self.rover_coupling_requirement = 3\n",
        "    self.excavator_observation_radius = 15\n",
        "    self.rover_observation_radius = 10\n",
        "    self.drone_observation_radius = 20\n",
        "    self.num_marked = 0\n",
        "    self.num_excavated = 0\n",
        "    self.num_communicated = 0\n",
        "\n",
        "\n",
        "  def mark_site(self, site):\n",
        "      rovs = 0\n",
        "      for rover in self.R:\n",
        "          if np.linalg.norm(rover.position - site.position) <= self.rover_observation_radius:\n",
        "              rovs += 1\n",
        "      if rovs >= self.rover_coupling_requirement:\n",
        "          site.type = 'Marked'\n",
        "          self.num_marked += 1\n",
        "\n",
        "  def excavate_site(self, site):\n",
        "      excs = 0\n",
        "      for excavator in self.E:\n",
        "          if np.linalg.norm(excavator.position - site.position) <= self.excavator_observation_radius:\n",
        "              excs += 1\n",
        "      if excs >= self.excavator_coupling_requirement:\n",
        "          site.excavated = True\n",
        "          self.num_excavated += 1\n",
        "\n",
        "\n",
        "  def rollout(self, digsites, num_iterations):\n",
        "    agents = self.R + self.E + self.D\n",
        "    for t in range(num_iterations):\n",
        "      for agent in agents:\n",
        "        x = agent.get_next_move(digsites,agents)\n",
        "        print(x)\n",
        "      for site in digsites:\n",
        "        self.mark_site(site)\n",
        "        if(site.type == \"Marked\"):\n",
        "          self.excavate_site(site)\n",
        "      if self.game_mode is None: pass\n",
        "      elif self.game_mode==\"D\": Decay(digsites)\n",
        "      elif self.game_mode==\"V\": Volatile(digsites)\n",
        "      else:\n",
        "        Decay(digsites)\n",
        "        Volatile(digsites)\n",
        "\n",
        "  def calc_fitness(self,digsites_updated):\n",
        "    fitness = 0\n",
        "    for dig_site in digsites_updated:\n",
        "        # Implement equation 4\n",
        "\n",
        "      if(dig_site.excavated == False):\n",
        "        continue\n",
        "      drones_covered = 0  # Number of drones covering the dig site\n",
        "      for drone in self.D:\n",
        "          distance = np.linalg.norm(drone.position - dig_site.position)\n",
        "          if distance <= self.drone_observation_radius:\n",
        "              drones_covered += 1\n",
        "\n",
        "      if drones_covered > 0:\n",
        "          fitness += dig_site.value\n",
        "          self.num_communicated += 1\n",
        "\n",
        "\n",
        "    self.fitness = fitness\n",
        "\n",
        "  def get_fitness(self, digsites):\n",
        "    digsites_updated = copy.deepcopy(digsites)\n",
        "    if self.fitness is not None:\n",
        "      return\n",
        "    else:\n",
        "      self.rollout(digsites_updated,50)\n",
        "      self.calc_fitness(digsites_updated)\n",
        "\n",
        "#############################################################################################################################################\n",
        "\n",
        "class Mainland:\n",
        "\n",
        "  def __init__(self, pop_size, team_size, elite_size, X_max, Y_max, num_digsites,game_mode):\n",
        "    self.ts = team_size\n",
        "    self.ps = pop_size\n",
        "    self.dim = [X_max, Y_max]\n",
        "    self.dss = num_digsites\n",
        "    self.es = elite_size\n",
        "    self.sites = []\n",
        "    self.population = []\n",
        "    self.rover_score = 0\n",
        "    self.excavator_score = 0\n",
        "    self.drone_score = 0\n",
        "    self.game_mode = game_mode # D for decay, V for volatile, M\n",
        "\n",
        "  def place_digsites(self):\n",
        "    for _ in range(self.dss):\n",
        "      position = [random.randint(0,self.dim[0]), random.randint(0,self.dim[1])]\n",
        "      value = np.random.randint(1,10)\n",
        "      # find quad\n",
        "      x, y = self.dim\n",
        "      if(position[0] <= x//2):\n",
        "        if(position[1] <= y //2):\n",
        "            quadrant = 1\n",
        "        else:\n",
        "          quadrant = 3\n",
        "      else:\n",
        "          if(position[1] <= y //2):\n",
        "            quadrant = 2\n",
        "          else:\n",
        "            quadrant = 4\n",
        "      self.sites.append(Site(position, value, quadrant))\n",
        "\n",
        "  def update_teams(self, All_r, All_e, All_d):\n",
        "    # input is list of policies of rovers, excavators, drones\n",
        "    All_r = [Rover(self.dim[0], self.dim[1], i) for i in All_r]\n",
        "    All_e = [Excavator(self.dim[0], self.dim[1], i) for i in All_e]\n",
        "    All_d = [Drone(self.dim[0], self.dim[1], i) for i in All_d]\n",
        "    All_agents = copy.deepcopy(All_r + All_e + All_d)\n",
        "    teams = [Team(random.choices(All_agents, k=self.ts), self.game_mode) for _ in range(self.ps)]\n",
        "    self.population = teams\n",
        "\n",
        "  def initialize(self):\n",
        "    # input is list of policies of rovers, excavators, drones\n",
        "    self.place_digsites()\n",
        "\n",
        "  def crossover_possible(self,T1, T2):\n",
        "    # atleast one agent class in common\n",
        "    ret = ((len(T1.R)>0) and (len(T2.R)>0)) or ((len(T1.E)>0) and (len(T2.E)>0)) or ((len(T1.D)>0) and (len(T2.D)>0))\n",
        "    return ret\n",
        "\n",
        "  def crossover(self, T_elite, T_nonelite):\n",
        "    # cross over T_e and T_ne\n",
        "    ## from T_ne, and 2 children return the one with the highest fitness [tournament selection]\n",
        "    while True:\n",
        "      rng = random.randint(1,3)\n",
        "      if rng==1:\n",
        "        # exchange rover\n",
        "        if not ((len(T_elite.R)>0) and (len(T_nonelite.R)>0)):\n",
        "          continue\n",
        "        C1, C2 = copy.deepcopy(T_elite), copy.deepcopy(T_nonelite)\n",
        "        idx1, idx2 = random.choice(range(len(C1.R))), random.choice(range(len(C2.R)))\n",
        "        C1.R[idx1], C2.R[idx2] = C2.R[idx2], C1.R[idx1]\n",
        "      if rng==2:\n",
        "        # exchange excavator\n",
        "        if not ((len(T_elite.E)>0) and (len(T_nonelite.E)>0)):\n",
        "          continue\n",
        "        C1, C2 = copy.deepcopy(T_elite), copy.deepcopy(T_nonelite)\n",
        "        idx1, idx2 = random.choice(range(len(C1.E))), random.choice(range(len(C2.E)))\n",
        "        C1.E[idx1], C2.E[idx2] = C2.E[idx2], C1.E[idx1]\n",
        "      if rng==3:\n",
        "        # exchange drone\n",
        "        if not ((len(T_elite.D)>0) and (len(T_nonelite.D)>0)):\n",
        "          continue\n",
        "        C1, C2 = copy.deepcopy(T_elite), copy.deepcopy(T_nonelite)\n",
        "        idx1, idx2 = random.choice(range(len(C1.D))), random.choice(range(len(C2.D)))\n",
        "        C1.D[idx1], C2.D[idx2] = C2.D[idx2], C1.D[idx1]\n",
        "      break\n",
        "    C1.fitness, C2.fitness = None, None\n",
        "    C1.get_fitness(self.sites)\n",
        "    C2.get_fitness(self.sites)\n",
        "    l = [T_nonelite, C1, C2]\n",
        "    l.sort(key=lambda team: team.fitness, reverse=True)\n",
        "    return l[0]\n",
        "\n",
        "  # def parallel(self,team_in):\n",
        "  #   team_in.get_fitness(self.sites)\n",
        "  #   return team_in\n",
        "\n",
        "  def run_algo(self, N):\n",
        "    # algorithm 2\n",
        "    for gen in tqdm(range(N)):\n",
        "\n",
        "      # get fitness\n",
        "      for team in self.population:\n",
        "        team.get_fitness(self.sites)\n",
        "        self.rover_score += team.num_marked\n",
        "        self.excavator_score += team.num_excavated\n",
        "        self.drone_score += team.num_communicated\n",
        "      # with multiprocessing.Pool() as pool:\n",
        "      #   pop = pool.map(self.parallel, self.population)\n",
        "      # self.population = pop\n",
        "\n",
        "\n",
        "      # get the set E\n",
        "      self.population.sort(key=lambda team: team.fitness, reverse=True)\n",
        "      E = self.population[:self.es]\n",
        "      T_e = self.population[self.es+1:]\n",
        "\n",
        "      # cross over\n",
        "      S = []\n",
        "      for pi_y in T_e:\n",
        "        pi_x = random.choice(E)\n",
        "        while not self.crossover_possible(pi_x, pi_y):\n",
        "          pi_x = random.choice(E)\n",
        "        S.append(self.crossover(pi_x, pi_y))\n",
        "\n",
        "      # T <- S âˆª E\n",
        "      self.population = E + S\n",
        "\n",
        "  def get_elite(self):\n",
        "    for team in self.population: team.get_fitness(self.sites)\n",
        "    self.population.sort(key=lambda team: team.fitness, reverse=True)\n",
        "    E = self.population[:self.es]\n",
        "    return E"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrhKRsM_1I2e"
      },
      "outputs": [],
      "source": [
        "def init_weights(num_mainlands, num_islands):\n",
        "  global weights\n",
        "  weights = np.zeros((num_mainlands,num_islands))\n",
        "  prob = np.zeros((num_mainlands, num_islands))\n",
        "  for i in range(num_mainlands):\n",
        "    for j in range(num_islands):\n",
        "      weights[i][j] = 1/num_islands ## initially start with every island having equal weightage\n",
        "\n",
        "#############################################################################################################################################\n",
        "def distribution_from_weights(num_mainlands,num_islands):\n",
        "  global prob\n",
        "  prob = np.zeros((num_mainlands, num_islands))\n",
        "  for i in range(num_mainlands):\n",
        "    for j in range(num_islands):\n",
        "      prob[i][j] = math.exp(weights[i][j])/np.sum(np.exp(weights[:, j]))\n",
        "#############################################################################################################################################\n",
        "def get_gradient_matrix(i,j):\n",
        "  global weights\n",
        "  weight_tensor = torch.tensor(weights.copy(), requires_grad=True)\n",
        "  prob_tensor = torch.exp(weight_tensor) / torch.sum(torch.exp(weight_tensor), dim=0)\n",
        "  target_probability = prob_tensor[i][j]\n",
        "\n",
        "  # Compute the gradient using automatic differentiation\n",
        "  target_probability.backward()\n",
        "# The gradient is now stored in weights.grad\n",
        "  gradient = weight_tensor.grad\n",
        "  gradient_matrix = gradient.numpy()\n",
        "  gradient_vector = gradient_matrix[:,i]\n",
        "  return gradient_vector\n",
        "#############################################################################################################################################\n",
        "def update_weights(Mainlands, num_islands, alpha=1e-5, nu=0.01):\n",
        "    global weights\n",
        "    num_mainlands = len(Mainlands)\n",
        "    for j in range(num_islands):\n",
        "        update = np.zeros(num_mainlands)\n",
        "        for m in range(1, num_mainlands + 1):\n",
        "            gradient = get_gradient_matrix(m-1,j)  # âˆ‡ğ‘¤ğœ‡(ğ‘š,ğ‘–)\n",
        "            if(j == 0 ):\n",
        "              performance = Mainlands[m-1].rover_score  # ğ‘“ğ‘š,ğ‘–\n",
        "            elif(j == 1):\n",
        "              performance = Mainlands[m-1].excavator_score  # ğ‘“ğ‘š,ğ‘–\n",
        "            else:\n",
        "              performance = Mainlands[m-1].drone_score  # ğ‘“ğ‘š,ğ‘–\n",
        "            log_term = math.log(prob[m - 1][j])  # ğœˆğ‘™ğ‘œğ‘”ğœ‡(ğ‘š,ğ‘–)\n",
        "            update += gradient * (performance - nu * log_term)\n",
        "        weights[:,j] += alpha * update  # Apply the update\n",
        "#############################################################################################################################################\n",
        "def select_agents(num_mainlands, Islands):\n",
        "  net_distribution = []\n",
        "  num_islands = len(Islands)\n",
        "  for m in range(num_mainlands):\n",
        "    for i in range(num_islands):\n",
        "      if Islands[i].agent_type == \"Rover\":\n",
        "          rovers = random.choices(Islands[i].agents, k=int(len(Islands[i].agents)*prob[m][i]))\n",
        "      elif Islands[i].agent_type == \"Excavator\":\n",
        "          excavators = random.choices(Islands[i].agents, k=int(len(Islands[i].agents)*prob[m][i]))\n",
        "      else:\n",
        "          drones = random.choices(Islands[i].agents, k=int(len(Islands[i].agents)*prob[m][i]))\n",
        "\n",
        "    net_distribution.append([[k.model for k in rovers],[k.model for k in excavators],[k.model for k in drones]])\n",
        "\n",
        "  return net_distribution\n",
        "#############################################################################################################################################\n",
        "# Decay and Volatile\n",
        "def Decay(site_list):\n",
        "  for site in site_list:\n",
        "    if site.type==\"Unmarked\":\n",
        "      site.value *= 0.5\n",
        "def Volatile(site_list, curr_ts):\n",
        "  for site in site_list:\n",
        "    if (site.type==\"Marked\") and (site.excavated==False):\n",
        "      if curr_ts - site.time_marked > 7:\n",
        "        site.type = \"Unmarked\"\n",
        "        site.time_marked = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GuzLv45He35",
        "outputId": "1554ba4c-6685-4b51-fb0a-4e90d879fb26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.5]\n",
            " [0.5]]\n"
          ]
        }
      ],
      "source": [
        "# rollout and rollout_island has numIter hyperparameter [hardcoded]\n",
        "num_iterations = 2\n",
        "num_agents_per_island = 100\n",
        "num_mainlands = 2\n",
        "team_size = 30\n",
        "pop_size = 30\n",
        "elite_size = 10\n",
        "mainland_mode = None\n",
        "Island_m = Mainland(0,0,0,60, 60,15, None)\n",
        "Island_m.initialize()\n",
        "init_weights(num_mainlands,1)\n",
        "distribution_from_weights(2,1)\n",
        "print(prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "qSKl-dX70LYL",
        "outputId": "adf064f6-5d10-4375-d97f-521597b8a99b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/decomposition/_pca.py:642: RuntimeWarning: invalid value encountered in divide\n",
            "  self.explained_variance_ratio_ = self.explained_variance_ / total_var\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-5937cb762033>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0;31m# ğ‘ƒğ‘œğ‘_ğ¼ = islands(ğ‘ƒğ‘œğ‘_ğ¼)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mIs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mIsland_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mIs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_policies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIsland_m\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msites\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mIs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mIs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_latent_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0;31m# ğ‘‡_ğ‘€ = mainlands(ğ‘‡_ğ‘€)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-f0fc1d73b01d>\u001b[0m in \u001b[0;36mupdate_policies\u001b[0;34m(self, sites, agents, N)\u001b[0m\n\u001b[1;32m     25\u001b[0m       \u001b[0;31m# Algorithm 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;31m# perturb weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/random.py\u001b[0m in \u001b[0;36mchoice\u001b[0;34m(self, seq)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0;34m\"\"\"Choose a random element from a non-empty sequence.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;31m# raises IndexError if seq is empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_randbelow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "## This cell is for the policy migration code, which has 4 parts,\n",
        "## 1) defining a softmax function to get the distribution of agents\n",
        "## 2) Invoking N iterations of islands and mainlands\n",
        "## 3) Sending elite teams back to the islands\n",
        "## 4) Updating the softmax with a gradient\n",
        "\n",
        "# Algorithm 3\n",
        "## Initialize islands\n",
        "NUM_EX_ROV = 0\n",
        "Island_list = [Island(\"Rover\", num_agents_per_island), Island(\"Excavator\", NUM_EX_ROV), Island(\"Drone\", NUM_EX_ROV)]\n",
        "# Island_list = [Island(\"Rover\", num_agents_per_island)]\n",
        "for Is in Island_list:\n",
        "  Is.initialize_agents(Island_m.dim)\n",
        "\n",
        "## Initialize Mainlands\n",
        "Mainland_list = [Mainland(pop_size,team_size,elite_size, 60,60, np.random.randint(1,20), mainland_mode) for _ in range(num_mainlands)]\n",
        "for M in Mainland_list:\n",
        "  M.initialize()\n",
        "\n",
        "for k in range(num_iterations):\n",
        "\n",
        "  # ğ‘ƒğ‘œğ‘_ğ¼ = islands(ğ‘ƒğ‘œğ‘_ğ¼)\n",
        "  for Is in Island_list:\n",
        "    Is.update_policies(Island_m.sites,Is.agents, 50)\n",
        "    Is.update_latent_space()\n",
        "  # ğ‘‡_ğ‘€ = mainlands(ğ‘‡_ğ‘€)\n",
        "  for m, M in enumerate(Mainland_list):\n",
        "    agent_distribution = select_agents(num_mainlands,Island_list)\n",
        "    rovers = agent_distribution[m][0]\n",
        "    excavators = agent_distribution[m][1]\n",
        "    drones = agent_distribution[m][2]\n",
        "    M.update_teams(rovers, excavators,drones)\n",
        "    M.run_algo(2)\n",
        "\n",
        "  # ğ‘ƒğ‘œğ‘_ğ‘– â† ğ‘ƒğ‘œğ‘_ğ‘– âˆª ğ‘‡_(ğ‘š,ğ‘–)[0:ğ‘’] âˆ€ğ‘š âˆˆ M\n",
        "  for M in Mainland_list:\n",
        "    E = M.get_elite()\n",
        "    for elite in E:\n",
        "      Island_list[0].add_agents(Island_m.dim,elite.R)\n",
        "      Island_list[1].add_agents(Island_m.dim, elite.E)\n",
        "      Island_list[2].add_agents(Island_m.dim, elite.D)\n",
        "  for Is in Island_list:\n",
        "    Is.update_latent_space()\n",
        "  # ğ‘¤_[ğ‘˜+1,ğ‘–] â† update(ğ‘¤_[ğ‘˜,ğ‘–])\n",
        "  update_weights(Mainland_list,3)\n",
        "\n",
        "  for M in Mainland_list:\n",
        "      # /* Replace ( |ğ‘‡ | âˆ’ ğ‘’ ) teams by sampling islands */\n",
        "      # 13 ğ‘‡_ğ‘š â† ğ‘‡_ğ‘š[0:ğ‘’]âˆª(|ğ‘‡|âˆ’ğ‘’) âˆ¼ ğ‘¤_(ğ‘˜+1,ğ‘–), âˆ€ğ‘– âˆˆ I\n",
        "      Tm_0_e = M.get_elite()\n",
        "      # add new teams from islands\n",
        "      agent_distribution = select_agents(num_mainlands,Island_list)\n",
        "      rovers = agent_distribution[m][0]\n",
        "      excavators = agent_distribution[m][1]\n",
        "      drones = agent_distribution[m][2]\n",
        "      M.update_teams(rovers, excavators,drones)\n",
        "      M.population = M.population[:-1*len(E)]\n",
        "      # add elite teams back\n",
        "      M.population += Tm_0_e"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_mainland = Mainland_list[0]\n",
        "test_team = test_mainland.population[0]\n",
        "\n",
        "test_team.rollout(test_mainland.sites,10)"
      ],
      "metadata": {
        "id": "cCfIfD-Zvc1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCn8f8xpg_9h"
      },
      "outputs": [],
      "source": [
        "count = 0\n",
        "for i in Mainland_list[0].temp:\n",
        "  if i.fitness is None:\n",
        "    count += 1\n",
        "print(count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnE3f2iYgOVd"
      },
      "outputs": [],
      "source": [
        "count = 0\n",
        "for i in Mainland_list[0].population:\n",
        "  if i.fitness is None:\n",
        "    count += 1\n",
        "print(count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oF0DAYw3hzC"
      },
      "outputs": [],
      "source": [
        "# class Site:\n",
        "\n",
        "#   def __init__(self, position, value,quadrant):\n",
        "#     self.position = position\n",
        "#     self.value = value\n",
        "#     self.type = \"Unmarked\"\n",
        "#     self.quadrant = quadrant\n",
        "\n",
        "# class Game:\n",
        "\n",
        "#   def __init__(self, length, width, numSites):\n",
        "#     global Game_x, Game_y\n",
        "#     Game_x = length\n",
        "#     Game_y = width\n",
        "#     self.x = length\n",
        "#     self.y = width\n",
        "#     self.numSites = numSites\n",
        "#     self.sites = []\n",
        "#     self.habitat = np.zeros((self.x,self.y))\n",
        "\n",
        "\n",
        "#   def placeDigSites(self):\n",
        "#     for i in range(self.numSites):\n",
        "#       position = np.random.randint(0,self.x,(2,))\n",
        "#       value = np.random.randint(1,10)\n",
        "#       quadrant = 0\n",
        "#       if(position[0] <= self.x//2):\n",
        "#         if(position[1] <= self.y //2):\n",
        "#             quadrant = 1\n",
        "#         else:\n",
        "#           qaudrant = 3\n",
        "#       else:\n",
        "#           if(position[1] <= self.y //2):\n",
        "#             quadrant = 2\n",
        "#           else:\n",
        "#             quadrant = 4\n",
        "\n",
        "#       self.sites.append(Site(position,value,quadrant))\n",
        "#       self.habitat[position[0]][position[1]] =  1 ## Let's use 1 for unmarked and 2 for marked ?\n",
        "\n",
        "#   def DrawGame(self):\n",
        "#         # Create a Matplotlib figure and axis\n",
        "#     fig, ax = plt.subplots()\n",
        "\n",
        "#     # Set the grid size\n",
        "#     GRID_SIZE = 6\n",
        "#     BOLD_LINE_WIDTH = 4\n",
        "#     NORMAL_LINE_WIDTH = 1\n",
        "\n",
        "#     # Draw the vertical grid lines\n",
        "#     for x in range(0,self.x -1 , GRID_SIZE):\n",
        "#         if x == self.x //2 :\n",
        "#             ax.axvline(x, color='black', lw=BOLD_LINE_WIDTH)\n",
        "#         else:\n",
        "#             ax.axvline(x, color='black', lw=NORMAL_LINE_WIDTH)\n",
        "\n",
        "#     # Draw the horizontal grid lines\n",
        "#     for y in range(0,self.y -1, GRID_SIZE):\n",
        "#         if y == self.y //2:\n",
        "#             ax.axhline(y, color='black', lw=BOLD_LINE_WIDTH)\n",
        "#         else:\n",
        "#             ax.axhline(y, color='black', lw=NORMAL_LINE_WIDTH)\n",
        "\n",
        "#     for i in range(self.x):\n",
        "#       for j in range(self.y):\n",
        "#         if(self.habitat[i][j] == 1 ):\n",
        "#           ax.text(i, j, \"U\", fontsize=12, color=\"red\")\n",
        "\n",
        "#     # Set axis limits\n",
        "#     ax.set_xlim(0, self.x)\n",
        "#     ax.set_ylim(0, self.y)\n",
        "\n",
        "#     # Show the grid\n",
        "#     plt.grid(False)\n",
        "#     plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZOG8yrv5999"
      },
      "outputs": [],
      "source": [
        "\n",
        "Habitat = Game(60,60,5)\n",
        "Habitat.placeDigSites()\n",
        "\n",
        "Habitat.DrawGame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87Mwb13G6gKV"
      },
      "outputs": [],
      "source": [
        "Game_x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwBZrxNVu6T6"
      },
      "outputs": [],
      "source": [
        "for i in Habitat.sites:\n",
        "  print(i.quadrant)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "efasCpMbr4Ml"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}